{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.270906\n",
      "         Iterations: 28\n",
      "         Function evaluations: 54\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.230341\n",
      "         Iterations: 33\n",
      "         Function evaluations: 62\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Analysis of multidimesional optimisation algorithms, Task_2_2, Bestuzhev, C4132\"\"\"\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "\n",
    "\"\"\"Common for all algorithms funtions and global variables\"\"\"\n",
    "\n",
    "precision=0.001\n",
    "point_quantity=100\n",
    "gauss_noise_average=0.5             # for Gauss_rand in approximated_function\n",
    "gauss_noise_deviation=0.16          # for Gauss_rand in approximated_function\n",
    "\n",
    "\n",
    "\"\"\" create random function for optimisation \"\"\"\n",
    "def create_function (precision=0.001, point_quantity=100, gauss_noise_average=0.5,\n",
    "                    gauss_noise_deviation=0.16):  \n",
    "    rand_val_1=random.random()\n",
    "    rand_val_2=random.random()\n",
    "    gauss_rand=[random.normalvariate(gauss_noise_average, gauss_noise_deviation)\\\n",
    "                                     for i in range(point_quantity+1)]\n",
    "    x=[i/point_quantity for i in range(point_quantity+1)]\n",
    "    y=[rand_val_1*x[i]+rand_val_2+gauss_rand[i] for i in range(point_quantity+1)]\n",
    "    return ({'x':x,'y':y})\n",
    "\n",
    "\n",
    "approximated_func=create_function()    # Data to approximate\n",
    "\n",
    "\n",
    "\"\"\" linear regress func=a*x+b \"\"\"\n",
    "def linear_regress(a, b, approximated_func):\n",
    "    result = 0\n",
    "    for i in range(len(approximated_func['x'])):\n",
    "        result += (a * approximated_func['x'][i] + b - approximated_func['y'][i])**2\n",
    "    return(result)\n",
    "\n",
    "\"\"\" rational regress func=a/(1+x*b) \"\"\"\n",
    "def rational_regress(a, b, approximated_func):\n",
    "    result = 0\n",
    "    for i in range(len(approximated_func['x'])):\n",
    "        denominator = 1 + approximated_func['x'][i]*b\n",
    "        if denominator != 0:\n",
    "            result += (a/denominator - approximated_func['y'][i])**2\n",
    "        else:\n",
    "            result += (a/precision - approximated_func['y'][i])**2\n",
    "    return(result)\n",
    "\n",
    "\n",
    "\"\"\"Methods of optimization\"\"\"\n",
    "\n",
    "\"\"\"Simple-iteration method for linear regression f=a*x+b\"\"\"\n",
    "def brute_force_linear_regress (approximated_func):\n",
    "    a_min = -0.5                               #! temporary assumption for tuning\n",
    "    a_max = 1.5                               #! temporary assumption for tuning\n",
    "    b_min = -0.5                              #! temporary assumption for tuning\n",
    "    b_max = 1.5                              #! temporary assumption for tuning\n",
    "    step_a=(a_max-a_min)*precision\n",
    "    step_b=(b_max-b_min)*precision\n",
    "    a_opt=a_min\n",
    "    b_opt=b_min\n",
    "    a=a_min\n",
    "    b=b_min\n",
    "    f_min=linear_regress(a,b,approximated_func)\n",
    "    counter=1\n",
    "    f=f_min\n",
    "    while a<a_max:\n",
    "        b=b_min\n",
    "        while b<b_max:\n",
    "            b+=step_b\n",
    "            f=linear_regress(a,b,approximated_func)\n",
    "            counter+=1\n",
    "            if (f_min>f):\n",
    "                f_min=f\n",
    "                a_opt=a\n",
    "                b_opt=b\n",
    "        a+=step_a\n",
    "    return({'a_opimal':a_opt, 'b_opimal':b_opt, 'minimum':f_min, 'counter':counter})\n",
    "\n",
    "\"\"\"Simple-iteration method for rational regression f=a/(1+b*x)\"\"\"\n",
    "def brute_force_rational_regress (approximated_func):\n",
    "    a_min=0.2                                #! temporary assumption for tuning\n",
    "    a_max=1.5                                #! temporary assumption for tuning\n",
    "    b_min=-0.6                               #! temporary assumption for tuning\n",
    "    b_max=0.8                                #! temporary assumption for tuning\n",
    "    step_a=(a_max-a_min)*precision\n",
    "    step_b=(b_max-b_min)*precision\n",
    "    a_opt=a_min\n",
    "    b_opt=b_min\n",
    "    a=a_min\n",
    "    b=b_min\n",
    "    f_min=rational_regress(a,b,approximated_func)\n",
    "    counter=1\n",
    "    f=f_min\n",
    "    while a<a_max:\n",
    "        b=b_min\n",
    "        while b<b_max:\n",
    "            b+=step_b\n",
    "            f=rational_regress(a,b,approximated_func)\n",
    "            counter+=1\n",
    "            if (f_min>f):\n",
    "                f_min=f\n",
    "                a_opt=a\n",
    "                b_opt=b\n",
    "        a+=step_a\n",
    "    return({'a_opimal':a_opt, 'b_opimal':b_opt, 'minimum':f_min, 'counter':counter})\n",
    "\n",
    "\n",
    "\"\"\"Functions for one dimention optimisation in Gasuss method\"\"\"\n",
    "\n",
    "\"\"\"One dimentional brute-force optimisation of 'a' in linear f=a*x+b\"\"\"\n",
    "def gauss_linear_a(held_b, a_min=0, a_max=1):\n",
    "    step = precision\n",
    "    a = a_min\n",
    "    a_opt = a\n",
    "    f = linear_regress(a, held_b, approximated_func)\n",
    "    counter = 1\n",
    "    f_min = f\n",
    "    while a < a_max:\n",
    "            a += step\n",
    "            f = linear_regress(a, held_b, approximated_func)\n",
    "            counter += 1\n",
    "            if (f_min > f):\n",
    "                f_min = f\n",
    "                a_opt = a\n",
    "    return({'x_opimal': a_opt, 'minimum': f_min, 'counter': counter})\n",
    "\n",
    "\"\"\"One dimentional brute-force optimisation of 'b' in linear f=a*x+b\"\"\"\n",
    "def gauss_linear_b(held_a, b_min=0, b_max=1):\n",
    "    step = precision\n",
    "    b = b_min\n",
    "    b_opt = b\n",
    "    f = linear_regress(held_a, b, approximated_func)\n",
    "    counter = 1\n",
    "    f_min = f\n",
    "    while b < b_max:\n",
    "            b += step\n",
    "            f = linear_regress(held_a, b, approximated_func)\n",
    "            counter += 1\n",
    "            if (f_min > f):\n",
    "                f_min = f\n",
    "                b_opt = b\n",
    "    return({'x_opimal': b_opt, 'minimum': f_min, 'counter': counter})\n",
    "\n",
    "\"\"\"One dimentional brute-force optimisation of 'a' in rational f=1/(a+b*x)\"\"\"\n",
    "def gauss_rational_a(held_b, a_min=0, a_max=1):\n",
    "    step = precision\n",
    "    a = a_min\n",
    "    a_opt = a\n",
    "    f = rational_regress(a, held_b, approximated_func)\n",
    "    counter = 1\n",
    "    f_min = f\n",
    "    while a < a_max:\n",
    "            a += step\n",
    "            f = rational_regress(a, held_b, approximated_func)\n",
    "            counter += 1\n",
    "            if (f_min > f):\n",
    "                f_min = f\n",
    "                a_opt = a\n",
    "    return({'x_opimal': a_opt, 'minimum': f_min, 'counter': counter})\n",
    "\n",
    "\"\"\"One dimentional brute-force optimisation of 'b' in rational f=1/(a+b*x)\"\"\"\n",
    "def gauss_rational_b(held_a, b_min=0, b_max=1):\n",
    "    step = precision\n",
    "    b = b_min\n",
    "    b_opt = b\n",
    "    f = rational_regress(held_a, b, approximated_func)\n",
    "    counter = 1\n",
    "    f_min = f\n",
    "    while b < b_max:\n",
    "            b += step\n",
    "            f = rational_regress(held_a, b, approximated_func)\n",
    "            counter += 1\n",
    "            if (f_min > f):\n",
    "                f_min = f\n",
    "                b_opt = b\n",
    "    return({'x_opimal': b_opt, 'minimum': f_min, 'counter': counter})\n",
    "\n",
    "\n",
    "\"\"\"Gauss method for linear regression f=a/(1+b*x)\"\"\"\n",
    "\"\"\"Brute-force method is used for one dimention optimisation\"\"\"\n",
    "def gauss_linear_regress(approximated_func):\n",
    "    a_min = -0.5                               #! temporary assumption for tuning\n",
    "    a_max = 1.5                               #! temporary assumption for tuning\n",
    "    b_min = -0.5                              #! temporary assumption for tuning\n",
    "    b_max = 1.5                              #! temporary assumption for tuning\n",
    "    b = (b_min+b_max)/2\n",
    "    f = gauss_linear_a(b, a_min, a_max)\n",
    "    f_min = f\n",
    "    counter = f['counter']\n",
    "    a = f_min['x_opimal']\n",
    "    a_opt = a\n",
    "    b_opt = b\n",
    "    \"\"\"Difference between function values at iterations is less than precision\"\"\"\n",
    "    \"\"\"and quantity of function recalls less than brute-force metods recalls.\"\"\"\n",
    "    while(counter < (1/precision)**2):\n",
    "        f = gauss_linear_b(a, b_min, b_max)\n",
    "        b = f['x_opimal']\n",
    "        counter += f['counter']\n",
    "        if(f_min['minimum'] > f['minimum']):\n",
    "            f_min = f\n",
    "            if(abs(b_opt - b) < precision):\n",
    "                b_opt = b\n",
    "                break\n",
    "            b_opt = b\n",
    "        f = gauss_linear_a(b, a_min, a_max)\n",
    "        a = f['x_opimal']\n",
    "        counter += f['counter']\n",
    "        if(f_min['minimum'] > f['minimum']):\n",
    "            f_min = f\n",
    "            if(abs(a_opt - a) < precision):\n",
    "                a_opt=a\n",
    "                break\n",
    "            a_opt=a\n",
    "    return({'a_opimal': a_opt, 'b_opimal': b_opt, 'minimum': f_min['minimum'], 'counter': counter})\n",
    "\n",
    "\"\"\" Gauss method for rational regression f=a/(1+b*x)\"\"\"\n",
    "\"\"\"Brute-force method is used for one dimention optimisation\"\"\"\n",
    "def gauss_rational_regress(approximated_func):\n",
    "    a_min=-0.5                                #! temporary assumption for tuning\n",
    "    a_max=1.5                                #! temporary assumption for tuning\n",
    "    b_min=-0.6                               #! temporary assumption for tuning\n",
    "    b_max=1.5                                #! temporary assumption for tuning    \n",
    "    b = (b_min+b_max)/2\n",
    "    f = gauss_rational_a(b, a_min, a_max)\n",
    "    f_min = f\n",
    "    counter = f['counter']\n",
    "    a = f_min['x_opimal']\n",
    "    a_opt = a\n",
    "    b_opt = b\n",
    "    \"\"\"Difference between function values at iterations is less than precision\"\"\"\n",
    "    \"\"\"and quantity of function recalls less than brute-force metods recalls.\"\"\"\n",
    "    while(counter < (1/precision)**2):\n",
    "        f = gauss_rational_b(a, b_min, b_max)\n",
    "        b = f['x_opimal']\n",
    "        counter += f['counter']\n",
    "        if(f_min['minimum'] > f['minimum']):\n",
    "            f_min = f\n",
    "            if(abs(b_opt - b) < precision):\n",
    "                b_opt = b\n",
    "                break\n",
    "            b_opt = b\n",
    "        f = gauss_rational_a(b, a_min, a_max)\n",
    "        a = f['x_opimal']\n",
    "        counter += f['counter']\n",
    "        if(f_min['minimum'] > f['minimum']):\n",
    "            f_min = f\n",
    "            if(abs(a_opt - a) < precision):\n",
    "                a_opt=a\n",
    "                break\n",
    "            a_opt=a\n",
    "    return({'a_opimal': a_opt, 'b_opimal': b_opt, 'minimum': f_min['minimum'], 'counter': counter})\n",
    "\n",
    "\"\"\"rational regress for Nelder-Mead\"\"\"\n",
    "def rat_regr_NM(start_point):\n",
    "    result=0\n",
    "    for i in range(len(approximated_func['x'])):\n",
    "        result+=(start_point[0]/(1+approximated_func['x'][i]*start_point[1])-approximated_func['y'][i])**2\n",
    "    return (result)\n",
    "\n",
    "\"\"\"linear regress for Nelder-Mead\"\"\"\n",
    "def lin_regr_NM(start_point):\n",
    "    result=0\n",
    "    for i in range(len(approximated_func['x'])):\n",
    "        result+=(start_point[0]*approximated_func['x'][i]+start_point[1]-approximated_func['y'][i])**2\n",
    "    return (result)\n",
    "\n",
    "\"\"\"linear optimization by Nelder-Mead method\"\"\"\n",
    "def linear_nelder_mead():\n",
    "    start_point = np.array([0.5, 0.5])\n",
    "    lin_Nel_Mead = minimize(lin_regr_NM, start_point, method='nelder-mead', options={'xtol': precision, 'disp': True})\n",
    "    return(lin_Nel_Mead)\n",
    "\n",
    "\"\"\"rational optimization by Nelder-Mead method\"\"\"\n",
    "def rational_nelder_mead():\n",
    "    start_point = np.array([0.5, 0.5])\n",
    "    rat_Nel_Mead = minimize(rat_regr_NM, start_point, method='nelder-mead', options={'xtol': precision, 'disp': True})\n",
    "    return(rat_Nel_Mead)\n",
    "\n",
    "def output_brute_linear():\n",
    "    x = approximated_func['x']\n",
    "    y = approximated_func['y']\n",
    "    opt = brute_force_linear_regress(approximated_func)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('vect_size, (units)')\n",
    "    ax.set_ylabel('time, s')\n",
    "    pylab.xlim(0, 1)\n",
    "    pylab.ylim(min(y), max(y))\n",
    "    line1, = ax.plot(x, y, linewidth=2, label='Data')\n",
    "    line2, = ax.plot(x, [(opt['a_opimal']*i+opt['b_opimal']) for i in x], dashes=[10, 5, 10, 5],\n",
    "                     linewidth=2, label='Brute linear')\n",
    "    ax.legend(loc='lower right', borderaxespad=0.1)\n",
    "    fig.savefig('A:\\Information\\Kostya\\Master\\Brut_lin.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "def output_brute_rational():\n",
    "    x = approximated_func['x']\n",
    "    y = approximated_func['y']\n",
    "    opt = brute_force_rational_regress(approximated_func)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('vect_size, (units)')\n",
    "    ax.set_ylabel('time, s')\n",
    "    pylab.xlim(0, 1)\n",
    "    pylab.ylim(min(y), max(y))\n",
    "    line1, = ax.plot(x, y, linewidth=2, label='Data')\n",
    "    line2, = ax.plot(x, [opt['a_opimal']/(1+opt['b_opimal']*i) for i in x], dashes=[10, 5, 10, 5],\n",
    "                     linewidth=2, label='Brute rational')\n",
    "    ax.legend(loc='lower right', borderaxespad=0.1)\n",
    "    fig.savefig('A:\\Information\\Kostya\\Master\\Brut_rat.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def output_gauss_linear():\n",
    "    x = approximated_func['x']\n",
    "    y = approximated_func['y']\n",
    "    opt = gauss_linear_regress(approximated_func)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('vect_size, (units)')\n",
    "    ax.set_ylabel('time, s')\n",
    "    pylab.xlim(0, 1)\n",
    "    pylab.ylim(min(y), max(y))\n",
    "    line1, = ax.plot(x, y, linewidth=2, label='Data')\n",
    "    line2, = ax.plot(x, [(opt['a_opimal']*i+opt['b_opimal']) for i in x], dashes=[10, 5, 10, 5],\n",
    "                     linewidth=2, label='Gauss linear')\n",
    "    ax.legend(loc='lower right', borderaxespad=0.1)\n",
    "    fig.savefig('A:\\Information\\Kostya\\Master\\Gauss_lin.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def output_gauss_rational():\n",
    "    x = approximated_func['x']\n",
    "    y = approximated_func['y']\n",
    "    opt = gauss_rational_regress(approximated_func)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('vect_size, (units)')\n",
    "    ax.set_ylabel('time, s')\n",
    "    pylab.xlim(0, 1)\n",
    "    pylab.ylim(min(y), max(y))\n",
    "    line1, = ax.plot(x, y, linewidth=2, label='Data')\n",
    "    line2, = ax.plot(x, [opt['a_opimal']/(1+opt['b_opimal']*i) for i in x], dashes=[10, 5, 10, 5],\n",
    "                     linewidth=2, label='Gauss rational')\n",
    "    ax.legend(loc='lower right', borderaxespad=0.1)\n",
    "    fig.savefig('A:\\Information\\Kostya\\Master\\Gauss_rat.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\"\"\"Output data\"\"\"\n",
    "with open ('A:\\Information\\Kostya\\Master\\Science\\Alg_2_2_output.txt',\n",
    "           'w', encoding=\"utf-8\") as ouf:\n",
    "    ouf.write(\"Approximated data\\n\")\n",
    "    ouf.write(str(approximated_func))\n",
    "    ouf.write(\"\\nBrute-force linear\\n\")\n",
    "    ouf.write(str(brute_force_linear_regress(approximated_func)))\n",
    "    ouf.write(\"\\nBrute-force rational\\n\")\n",
    "    ouf.write(str(brute_force_rational_regress(approximated_func)))\n",
    "    ouf.write(\"\\nGauss linear\\n\")\n",
    "    ouf.write(str(gauss_linear_regress(approximated_func)))\n",
    "    ouf.write(\"\\nGauss rational\\n\")\n",
    "    ouf.write(str(gauss_rational_regress(approximated_func)))\n",
    "    ouf.write(\"\\nNelder-Mead linear\\n\")\n",
    "    ouf.write(str(linear_nelder_mead()))\n",
    "    ouf.write(\"\\nNelder-Mead rational\\n\")\n",
    "    ouf.write(str(rational_nelder_mead()))\n",
    "    \n",
    "#print(brute_force_linear_regress(approximated_func))\n",
    "#output_brute_linear()\n",
    "#print(brute_force_rational_regress(approximated_func))\n",
    "#output_brute_rational()\n",
    "#print(gauss_linear_regress(approximated_func))\n",
    "#output_gauss_linear()\n",
    "#print(gauss_rational_regress(approximated_func))\n",
    "#output_gauss_rational()\n",
    "#output_all_linear()\n",
    "#output_all_rational()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
